{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "pS0ShXNcrS1I"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Activation function and its derivatives\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))"
      ],
      "metadata": {
        "id": "JwmEE3x7rYKb"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid_derivative(x):\n",
        "    return sigmoid(x) * (1 - sigmoid(x))"
      ],
      "metadata": {
        "id": "doAwGknMraBs"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loss function and its derivative\n",
        "def binary_cross_entropy(y_true, y_pred):\n",
        "    return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
        "\n",
        "def binary_cross_entropy_derivative(y_true, y_pred):\n",
        "    return -(y_true / y_pred) + (1 - y_true) / (1 - y_pred)"
      ],
      "metadata": {
        "id": "kBRKjiMUrcEK"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Neural Network class\n",
        "class NeuralNetwork:\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        # Initialize weights and biases\n",
        "        self.weights1 = np.random.randn(input_size, hidden_size)\n",
        "        self.bias1 = np.zeros((1, hidden_size))\n",
        "        self.weights2 = np.random.randn(hidden_size, output_size)\n",
        "        self.bias2 = np.zeros((1, output_size))\n",
        "\n",
        "    def forward(self, X):\n",
        "        # Forward pass\n",
        "        self.z1 = np.dot(X, self.weights1) + self.bias1\n",
        "        self.a1 = sigmoid(self.z1)\n",
        "        self.z2 = np.dot(self.a1, self.weights2) + self.bias2\n",
        "        self.a2 = sigmoid(self.z2)\n",
        "        return self.a2\n",
        "\n",
        "    def backward(self, X, y, y_pred, learning_rate):\n",
        "        # Backward pass - dl/ds*ds/dz*dz/dw\n",
        "        loss_gradient = binary_cross_entropy_derivative(y, y_pred)\n",
        "\n",
        "        # Gradients for weights2 and bias2\n",
        "        dz2 = loss_gradient * sigmoid_derivative(self.z2)\n",
        "        dw2 = np.dot(self.a1.T, dz2)\n",
        "        db2 = np.sum(dz2, axis=0, keepdims=True)\n",
        "\n",
        "        # Gradients for weights1 and bias1\n",
        "        dz1 = np.dot(dz2, self.weights2.T) * sigmoid_derivative(self.z1)\n",
        "        dw1 = np.dot(X.T, dz1)\n",
        "        db1 = np.sum(dz1, axis=0, keepdims=True)\n",
        "\n",
        "        # Update weights and biases\n",
        "        self.weights1 -= learning_rate * dw1\n",
        "        self.bias1 -= learning_rate * db1\n",
        "        self.weights2 -= learning_rate * dw2\n",
        "        self.bias2 -= learning_rate * db2\n",
        "\n",
        "\n",
        "    def train(self, X, y, epochs, learning_rate):\n",
        "        for epoch in range(epochs):\n",
        "            # Forward pass\n",
        "            y_pred = self.forward(X)\n",
        "\n",
        "            # Compute loss\n",
        "            loss = binary_cross_entropy(y, y_pred)\n",
        "\n",
        "            # Backward pass\n",
        "            self.backward(X, y, y_pred, learning_rate)\n",
        "\n",
        "            # Print loss every 100 epochs\n",
        "            if epoch % 100 == 0:\n",
        "                print(f\"Epoch {epoch}, Loss: {loss}\")\n"
      ],
      "metadata": {
        "id": "jFUHXTpGrhA0"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Dummy dataset (XOR problem)\n",
        "    X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "    y = np.array([[0], [1], [1], [0]])\n",
        "\n",
        "    # Initialize and train the neural network\n",
        "    nn = NeuralNetwork(input_size=2, hidden_size=4, output_size=1)\n",
        "    nn.train(X, y, epochs=1000, learning_rate=0.1)\n",
        "\n",
        "    # Test the model\n",
        "    print(\"Predictions:\")\n",
        "    print(nn.forward(X))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vy1XfuiWrk3-",
        "outputId": "ebd336f1-6c4d-408f-c634-31369cfebbf0"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 0.8303997013474003\n",
            "Epoch 100, Loss: 0.6154736467900566\n",
            "Epoch 200, Loss: 0.45048805384888074\n",
            "Epoch 300, Loss: 0.2661484826807904\n",
            "Epoch 400, Loss: 0.16078889613718494\n",
            "Epoch 500, Loss: 0.10774458085787805\n",
            "Epoch 600, Loss: 0.07867478797790405\n",
            "Epoch 700, Loss: 0.06105111463048071\n",
            "Epoch 800, Loss: 0.04946273402102626\n",
            "Epoch 900, Loss: 0.041355987463375446\n",
            "Predictions:\n",
            "[[0.04194818]\n",
            " [0.96756114]\n",
            " [0.96613921]\n",
            " [0.03087176]]\n"
          ]
        }
      ]
    }
  ]
}